{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T22:14:45.186427Z",
     "start_time": "2025-05-31T22:14:44.586811Z"
    }
   },
   "source": [
    "# notebooks/model_evaluation.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Setup paths and directories\n",
    "NOTEBOOKS_DIR = os.path.dirname(os.path.abspath(''))\n",
    "RESULTS_DIR = os.path.join(NOTEBOOKS_DIR, 'evaluation_results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize results file\n",
    "results_file = os.path.join(RESULTS_DIR, 'evaluation_summary.txt')\n",
    "with open(results_file, 'w') as f:\n",
    "    f.write(\"UFC Fight Predictor - Model Evaluation Report\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "def save_result(section_title, content):\n",
    "    \"\"\"Save results to text file with section formatting\"\"\"\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"\\n{section_title}\\n\")\n",
    "        f.write(\"-\"*len(section_title) + \"\\n\\n\")\n",
    "        if isinstance(content, pd.DataFrame):\n",
    "            f.write(content.to_string() + \"\\n\\n\")\n",
    "        else:\n",
    "            f.write(str(content) + \"\\n\\n\")\n",
    "\n",
    "# Add parent directory to path to import modules\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import utility functions\n",
    "from ufc_predictor.utils import get_data_path, preprocess_data, get_fighter_stats\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Load Model and Data\n",
    "\n",
    "# %%\n",
    "# Load the trained model\n",
    "model_path = os.path.join(os.path.abspath('..'), 'models', 'ufc_predictor_v1.pkl')\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(get_data_path())\n",
    "df_processed = preprocess_data(df)\n",
    "\n",
    "# Save dataset info\n",
    "dataset_info = f\"\"\"\n",
    "Dataset Information:\n",
    "- Original shape: {df.shape}\n",
    "- Processed shape: {df_processed.shape}\n",
    "- Features: {len(df_processed.columns) - 1}\n",
    "- Fights: {len(df_processed)}\n",
    "- Time period: {df['Date'].min()} to {df['Date'].max()}\n",
    "\"\"\"\n",
    "save_result(\"DATASET INFORMATION\", dataset_info)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop(columns=['Target'])\n",
    "y = df_processed['Target']\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Make Predictions\n",
    "\n",
    "# %%\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X)\n",
    "y_pred_proba = model.predict_proba(X)[:, 1]  # Probability of Red winning\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Basic Performance Metrics\n",
    "\n",
    "# %%\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y, y_pred, target_names=['Blue Win', 'Red Win'])\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    index=['Actual Blue', 'Actual Red'],\n",
    "                    columns=['Predicted Blue', 'Predicted Red'])\n",
    "\n",
    "# Save results\n",
    "save_result(\"ACCURACY\", f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "save_result(\"CLASSIFICATION REPORT\", class_report)\n",
    "save_result(\"CONFUSION MATRIX\", cm_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Advanced Metrics\n",
    "\n",
    "# %%\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y, y_pred_proba)\n",
    "avg_precision = average_precision_score(y, y_pred_proba)\n",
    "\n",
    "# Save results\n",
    "roc_info = f\"\"\"\n",
    "ROC Curve Analysis:\n",
    "- AUC Score: {roc_auc:.4f}\n",
    "- Interpretation: {'Excellent (>0.9)' if roc_auc > 0.9 else\n",
    "                   'Good (0.8-0.9)' if roc_auc > 0.8 else\n",
    "                   'Fair (0.7-0.8)' if roc_auc > 0.7 else\n",
    "                   'Poor (<0.7)'}\n",
    "\"\"\"\n",
    "pr_info = f\"\"\"\n",
    "Precision-Recall Analysis:\n",
    "- Average Precision: {avg_precision:.4f}\n",
    "- Baseline: {len(y[y==1])/len(y):.4f} (class distribution)\n",
    "\"\"\"\n",
    "save_result(\"ROC CURVE ANALYSIS\", roc_info)\n",
    "save_result(\"PRECISION-RECALL ANALYSIS\", pr_info)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Feature Importance Analysis\n",
    "\n",
    "# %%\n",
    "feature_importances = \"\"\n",
    "if hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "    try:\n",
    "        # Get feature names\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        feature_names = preprocessor.transformers_[0][2]\n",
    "\n",
    "        # Get feature importances\n",
    "        importances = model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "        # Create importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "        feature_importances = importance_df\n",
    "    except Exception as e:\n",
    "        feature_importances = f\"Error extracting feature importances: {str(e)}\"\n",
    "else:\n",
    "    feature_importances = \"Feature importances not available for this model type\"\n",
    "\n",
    "save_result(\"TOP 10 FEATURE IMPORTANCES\", feature_importances)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Prediction Distribution\n",
    "\n",
    "# %%\n",
    "# Analyze prediction distribution\n",
    "prob_df = pd.DataFrame({\n",
    "    'Predicted Probability': y_pred_proba,\n",
    "    'Actual Outcome': y.map({0: 'Blue Win', 1: 'Red Win'})\n",
    "})\n",
    "\n",
    "# Calculate calibration\n",
    "prob_df['Probability Bin'] = pd.cut(prob_df['Predicted Probability'],\n",
    "                                    bins=np.arange(0, 1.1, 0.1),\n",
    "                                    include_lowest=True)\n",
    "\n",
    "calibration = prob_df.groupby('Probability Bin')['Actual Outcome'].value_counts(\n",
    "    normalize=True).unstack().fillna(0)\n",
    "calibration['Total Predictions'] = prob_df.groupby('Probability Bin').size()\n",
    "\n",
    "save_result(\"PREDICTION DISTRIBUTION\", prob_df['Predicted Probability'].describe())\n",
    "save_result(\"CALIBRATION ANALYSIS\", calibration)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Performance Over Time\n",
    "\n",
    "# %%\n",
    "# Add fight date back for temporal analysis\n",
    "df_full = pd.read_csv(get_data_path())\n",
    "df_full['Date'] = pd.to_datetime(df_full['Date'])\n",
    "df_full['Year'] = df_full['Date'].dt.year\n",
    "df_full['Prediction'] = y_pred\n",
    "df_full['Correct'] = (df_full['Winner'].map({'Red': 1, 'Blue': 0}) == df_full['Prediction']).astype(int)\n",
    "\n",
    "# Calculate yearly accuracy\n",
    "yearly_accuracy = df_full.groupby('Year')['Correct'].mean().reset_index()\n",
    "yearly_accuracy.columns = ['Year', 'Accuracy']\n",
    "\n",
    "# Calculate fight count per year\n",
    "yearly_counts = df_full['Year'].value_counts().sort_index().reset_index()\n",
    "yearly_counts.columns = ['Year', 'Fight Count']\n",
    "\n",
    "# Combine results\n",
    "yearly_performance = yearly_accuracy.merge(yearly_counts, on='Year')\n",
    "save_result(\"YEARLY PERFORMANCE\", yearly_performance)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Historical Superfight Predictions\n",
    "\n",
    "# %%\n",
    "def predict_fight(red_fighter, blue_fighter):\n",
    "    \"\"\"Predict a fight between two fighters\"\"\"\n",
    "    # Get fighter stats\n",
    "    red_stats = get_fighter_stats(red_fighter)\n",
    "    blue_stats = get_fighter_stats(blue_fighter)\n",
    "\n",
    "    if not red_stats or not blue_stats:\n",
    "        return None\n",
    "\n",
    "    # Create input DataFrame\n",
    "    input_data = pd.DataFrame([{\n",
    "        'RedOdds': -150,\n",
    "        'BlueOdds': 130,\n",
    "        'WinStreakDif': red_stats.get('win_streak', 0) - blue_stats.get('win_streak', 0),\n",
    "        'RedAge': red_stats.get('age', 30),\n",
    "        'BlueAge': blue_stats.get('age', 30),\n",
    "        'NumberOfRounds': 3,\n",
    "        'TitleBout': 0,\n",
    "        'RedHeightCms': red_stats.get('height', 180),\n",
    "        'BlueHeightCms': blue_stats.get('height', 180),\n",
    "        'RedReachCms': red_stats.get('reach', 180),\n",
    "        'BlueReachCms': blue_stats.get('reach', 180),\n",
    "        'RedStance': red_stats.get('stance', 'Orthodox'),\n",
    "        'BlueStance': blue_stats.get('stance', 'Orthodox')\n",
    "    }])\n",
    "\n",
    "    # Create features\n",
    "    input_data['OddsRatio'] = input_data['RedOdds'] / input_data['BlueOdds']\n",
    "    input_data['HeightAdvRed'] = input_data['RedHeightCms'] - input_data['BlueHeightCms']\n",
    "    input_data['ReachAdvRed'] = input_data['RedReachCms'] - input_data['BlueReachCms']\n",
    "    input_data['SizeAdvRed'] = (input_data['HeightAdvRed'] + input_data['ReachAdvRed']) / 2\n",
    "    input_data['StanceMatch'] = (input_data['RedStance'] == input_data['BlueStance']).astype(int)\n",
    "\n",
    "    # Select features for model\n",
    "    model_features = model.named_steps['preprocessor'].transformers_[0][2]\n",
    "    model_input = input_data[model_features]\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(model_input)[0]\n",
    "    prediction_proba = model.predict_proba(model_input)[0]\n",
    "\n",
    "    return {\n",
    "        'red_win_prob': prediction_proba[1],\n",
    "        'blue_win_prob': prediction_proba[0],\n",
    "        'predicted_winner': 'Red' if prediction == 1 else 'Blue'\n",
    "    }\n",
    "\n",
    "# Define historical superfights\n",
    "superfights = [\n",
    "    {\"red\": \"Conor McGregor\", \"blue\": \"Khabib Nurmagomedov\", \"actual\": \"Blue\"},\n",
    "    {\"red\": \"Jon Jones\", \"blue\": \"Daniel Cormier\", \"actual\": \"Red\"},\n",
    "    {\"red\": \"Israel Adesanya\", \"blue\": \"Alex Pereira\", \"actual\": \"Blue\"},\n",
    "    {\"red\": \"Ronda Rousey\", \"blue\": \"Holly Holm\", \"actual\": \"Blue\"},\n",
    "    {\"red\": \"Georges St-Pierre\", \"blue\": \"Michael Bisping\", \"actual\": \"Red\"},\n",
    "    {\"red\": \"Anderson Silva\", \"blue\": \"Chael Sonnen\", \"actual\": \"Red\"},\n",
    "    {\"red\": \"Nate Diaz\", \"blue\": \"Conor McGregor\", \"actual\": \"Blue\"},\n",
    "    {\"red\": \"Amanda Nunes\", \"blue\": \"Ronda Rousey\", \"actual\": \"Red\"}\n",
    "]\n",
    "\n",
    "# Test superfights\n",
    "results = []\n",
    "for fight in superfights:\n",
    "    result = predict_fight(fight[\"red\"], fight[\"blue\"])\n",
    "    if result:\n",
    "        correct = fight['actual'][0] == result['predicted_winner'][0]\n",
    "        results.append({\n",
    "            \"Red Fighter\": fight[\"red\"],\n",
    "            \"Blue Fighter\": fight[\"blue\"],\n",
    "            \"Actual Winner\": fight['actual'],\n",
    "            \"Predicted Winner\": result['predicted_winner'],\n",
    "            \"Red Win Prob\": f\"{result['red_win_prob']*100:.1f}%\",\n",
    "            \"Blue Win Prob\": f\"{result['blue_win_prob']*100:.1f}%\",\n",
    "            \"Correct\": correct\n",
    "        })\n",
    "\n",
    "# Create results table\n",
    "results_df = pd.DataFrame(results)\n",
    "superfight_accuracy = results_df['Correct'].mean()\n",
    "\n",
    "# Save results\n",
    "save_result(\"HISTORICAL SUPERFIGHT PREDICTIONS\", results_df)\n",
    "save_result(\"SUPERFIGHT ACCURACY\",\n",
    "           f\"Accuracy on historical superfights: {superfight_accuracy*100:.1f}%\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Betting Strategy Simulation\n",
    "\n",
    "# %%\n",
    "# Add actual odds to our evaluation data\n",
    "df_full = pd.read_csv(get_data_path())\n",
    "df_eval = df_processed.copy()\n",
    "df_eval['RedOddsActual'] = df_full['RedOdds']\n",
    "df_eval['BlueOddsActual'] = df_full['BlueOdds']\n",
    "df_eval['PredictedWinProb'] = y_pred_proba\n",
    "\n",
    "# Filter only fights where we have odds\n",
    "df_eval = df_eval.dropna(subset=['RedOddsActual', 'BlueOddsActual'])\n",
    "\n",
    "# Calculate implied probabilities from odds\n",
    "def odds_to_prob(odds):\n",
    "    if odds > 0:\n",
    "        return 100 / (odds + 100)\n",
    "    else:\n",
    "        return -odds / (-odds + 100)\n",
    "\n",
    "df_eval['RedImpliedProb'] = df_eval['RedOddsActual'].apply(odds_to_prob)\n",
    "df_eval['BlueImpliedProb'] = df_eval['BlueOddsActual'].apply(odds_to_prob)\n",
    "\n",
    "# Define betting strategy\n",
    "def betting_strategy(row):\n",
    "    \"\"\"Value betting strategy\"\"\"\n",
    "    if row['PredictedWinProb'] > row['RedImpliedProb'] + 0.05:\n",
    "        return 'Red'\n",
    "    elif (1 - row['PredictedWinProb']) > row['BlueImpliedProb'] + 0.05:\n",
    "        return 'Blue'\n",
    "    else:\n",
    "        return 'No Bet'\n",
    "\n",
    "df_eval['BetOn'] = df_eval.apply(betting_strategy, axis=1)\n",
    "\n",
    "# Calculate returns\n",
    "def calculate_return(row):\n",
    "    if row['BetOn'] == 'No Bet':\n",
    "        return 0\n",
    "    elif row['BetOn'] == 'Red' and row['Target'] == 1:\n",
    "        return (100 / row['RedImpliedProb']) - 100 if row['RedOddsActual'] < 0 else row['RedOddsActual']\n",
    "    elif row['BetOn'] == 'Blue' and row['Target'] == 0:\n",
    "        return (100 / row['BlueImpliedProb']) - 100 if row['BlueOddsActual'] < 0 else row['BlueOddsActual']\n",
    "    else:\n",
    "        return -100\n",
    "\n",
    "df_eval['Return'] = df_eval.apply(calculate_return, axis=1)\n",
    "\n",
    "# Analyze results\n",
    "total_bets = len(df_eval[df_eval['BetOn'] != 'No Bet'])\n",
    "total_stake = total_bets * 100\n",
    "total_return = df_eval['Return'].sum()\n",
    "roi = (total_return / total_stake) * 100 if total_bets > 0 else 0\n",
    "win_percentage = len(df_eval[(df_eval['Return'] > 0) & (df_eval['BetOn'] != 'No Bet')]) / total_bets\n",
    "\n",
    "# Save results\n",
    "betting_results = f\"\"\"\n",
    "BETTING STRATEGY RESULTS:\n",
    "- Total fights with odds: {len(df_eval)}\n",
    "- Bets placed: {total_bets}\n",
    "- Win rate: {win_percentage*100:.1f}%\n",
    "- Total return: ${total_return:.2f}\n",
    "- Total stake: ${total_stake:.2f}\n",
    "- ROI: {roi:.1f}%\n",
    "\"\"\"\n",
    "save_result(\"BETTING SIMULATION RESULTS\", betting_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Final Summary\n",
    "\n",
    "# %%\n",
    "# Create final summary\n",
    "final_summary = f\"\"\"\n",
    "FINAL MODEL EVALUATION SUMMARY:\n",
    "\n",
    "1. MODEL PERFORMANCE:\n",
    "   - Accuracy: {accuracy:.4f}\n",
    "   - ROC AUC: {roc_auc:.4f}\n",
    "   - Avg Precision: {avg_precision:.4f}\n",
    "   - Superfight Accuracy: {superfight_accuracy:.4f}\n",
    "\n",
    "2. BETTING PERFORMANCE:\n",
    "   - ROI: {roi:.1f}%\n",
    "   - Win Rate: {win_percentage*100:.1f}%\n",
    "   - Total Return: ${total_return:.2f} (on ${total_stake:.2f} stake)\n",
    "\n",
    "3. RECOMMENDATIONS:\n",
    "   - {\"‚úÖ STRONG MODEL: Ready for deployment\" if roi > 10 else\n",
    "      \"‚ö†Ô∏è MODERATE MODEL: Needs minor improvements\" if roi > 0 else\n",
    "      \"‚ùå WEAK MODEL: Requires significant improvements\"}\n",
    "\"\"\"\n",
    "\n",
    "# Save final summary\n",
    "with open(os.path.join(RESULTS_DIR, 'FINAL_SUMMARY.txt'), 'w') as f:\n",
    "    f.write(final_summary)\n",
    "\n",
    "# Print completion message\n",
    "print(\"‚úÖ Evaluation complete!\")\n",
    "print(f\"üìù Results saved to: {RESULTS_DIR}\")\n",
    "print(final_summary)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hd/3clcp0nn0l770pht86xvts6r0000gn/T/ipykernel_1755/4039648871.py:170: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  calibration = prob_df.groupby('Probability Bin')['Actual Outcome'].value_counts(\n",
      "/var/folders/hd/3clcp0nn0l770pht86xvts6r0000gn/T/ipykernel_1755/4039648871.py:172: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  calibration['Total Predictions'] = prob_df.groupby('Probability Bin').size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation complete!\n",
      "üìù Results saved to: /Users/sukhmandeep/PycharmProjects/ufc-fight-predictor/evaluation_results\n",
      "\n",
      "FINAL MODEL EVALUATION SUMMARY:\n",
      "\n",
      "1. MODEL PERFORMANCE:\n",
      "   - Accuracy: 0.6578\n",
      "   - ROC AUC: 0.7328\n",
      "   - Avg Precision: 0.7919\n",
      "   - Superfight Accuracy: 0.3750\n",
      "\n",
      "2. BETTING PERFORMANCE:\n",
      "   - ROI: 26.4%\n",
      "   - Win Rate: 53.1%\n",
      "   - Total Return: $48500.34 (on $183900.00 stake)\n",
      "\n",
      "3. RECOMMENDATIONS:\n",
      "   - ‚úÖ STRONG MODEL: Ready for deployment\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
